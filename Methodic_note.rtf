{\rtf1\ansi\deff3\adeflang1025
{\fonttbl{\f0\froman\fprq2\fcharset0 Times New Roman;}{\f1\froman\fprq2\fcharset2 Symbol;}{\f2\fswiss\fprq2\fcharset0 Arial;}{\f3\froman\fprq2\fcharset0 Liberation Serif{\*\falt Times New Roman};}{\f4\fswiss\fprq2\fcharset0 Liberation Sans{\*\falt Arial};}{\f5\froman\fprq2\fcharset0 Calibri;}{\f6\fnil\fprq2\fcharset0 Microsoft YaHei;}{\f7\fnil\fprq2\fcharset0 Times New Roman;}{\f8\fswiss\fprq0\fcharset128 Lucida Sans;}{\f9\fnil\fprq2\fcharset0 Lucida Sans;}{\f10\fnil\fprq2\fcharset0 Calibri;}}
{\colortbl;\red0\green0\blue0;\red0\green0\blue255;\red0\green255\blue255;\red0\green255\blue0;\red255\green0\blue255;\red255\green0\blue0;\red255\green255\blue0;\red255\green255\blue255;\red0\green0\blue128;\red0\green128\blue128;\red0\green128\blue0;\red128\green0\blue128;\red128\green0\blue0;\red128\green128\blue0;\red128\green128\blue128;\red192\green192\blue192;}
{\stylesheet{\s0\snext0\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\sl256\slmult1\ql\widctlpar\faauto\sb0\sa160\hyphpar0\ltrpar\loch\f5\fs22\lang2057\cf0\kerning1 Normal;}
{\*\cs15\snext15 Default Paragraph Font;}
{\*\cs16\sbasedon15\snext16\dbch\af7\afs16\fs16 annotation reference;}
{\*\cs17\sbasedon15\snext17\dbch\af7\afs20\fs20\lang1033 Comment Text Char;}
{\*\cs18\sbasedon17\snext18\dbch\af7\afs20\ab\fs20\lang1033\b Comment Subject Char;}
{\s19\sbasedon0\snext20\dbch\af6\langfe1036\dbch\af9\afs28\alang1025\sl256\slmult1\ql\widctlpar\faauto\sb240\sa120\keepn\loch\f4\fs28\lang2057 Heading;}
{\s20\sbasedon0\snext20\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\sl276\slmult1\ql\widctlpar\faauto\sb0\sa140\loch\f5\fs22\lang2057 Text Body;}
{\s21\sbasedon20\snext21\dbch\af7\langfe1036\dbch\af8\afs22\alang1025\sl276\slmult1\ql\widctlpar\faauto\sb0\sa140\loch\f5\fs22\lang2057 List;}
{\s22\sbasedon0\snext22\dbch\af7\langfe1036\dbch\af8\afs24\alang1025\ai\sl256\slmult1\ql\widctlpar\faauto\sb120\sa120\noline\loch\f5\fs24\lang2057\i Caption;}
{\s23\sbasedon0\snext23\dbch\af7\langfe1036\dbch\af8\afs22\alang1025\sl256\slmult1\ql\widctlpar\faauto\sb0\sa160\noline\loch\f5\fs22\lang2057 Index;}
{\s24\snext24\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\sl256\slmult1\ql\widctlpar\faauto\sb0\sa160\hyphpar0\ltrpar\loch\f5\fs22\lang1036\cf0\kerning1 DocumentMap;}
{\s25\sbasedon0\snext25\dbch\af7\langfe1036\dbch\af10\afs20\alang1025\sl240\slmult1\ql\widctlpar\faauto\sb0\sa160\loch\f5\fs20\lang2057 annotation text;}
{\s26\sbasedon25\snext26\dbch\af7\langfe1036\dbch\af10\afs20\alang1025\ab\sl240\slmult1\ql\widctlpar\faauto\sb0\sa160\loch\f5\fs20\lang2057\b annotation subject;}
{\s27\snext27\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ql\widctlpar\faauto\hyphpar0\ltrpar\loch\f5\fs22\lang1033\cf0\kerning1 Revision;}
}{\*\generator LibreOffice/6.3.3.2$Windows_X86_64 LibreOffice_project/a64200df03143b798afd1ec74a12ab50359878ed}{\info{\creatim\yr2022\mo2\dy9\hr12\min57}{\revtim\yr2022\mo2\dy13\hr1\min16}{\printim\yr0\mo0\dy0\hr0\min0}}{\*\userprops{\propname Operator}\proptype30{\staticval Morenkov, Oleg}}\deftab720\deftab720\deftab720
\viewscale130
{\*\pgdsctbl
{\pgdsc0\pgdscuse451\pgwsxn12240\pghsxn15840\marglsxn1440\margrsxn1440\margtsxn1440\margbsxn1440\pgdscnxt0 Default Style;}}
\formshade{\*\pgdscno0}\paperh15840\paperw12240\margl1440\margr1440\margt1440\margb1440\sectd\sbknone\sectunlocked1\pgndec\pgwsxn12240\pghsxn15840\marglsxn1440\margrsxn1440\margtsxn1440\margbsxn1440\ftnbj\ftnstart1\ftnrstcont\ftnnar\aenddoc\aftnrstcont\aftnstart1\aftnnrlc
{\*\ftnsep\chftnsep}\pgndec\pard\plain \s0\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\sl256\slmult1\ql\widctlpar\faauto\sb0\sa160\hyphpar0\ltrpar\loch\f5\fs22\lang2057\cf0\kerning1\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa200\ltrpar{\rtlch\afs32\ab \ltrch\fs32\lang1033\b\loch
Exploratory data analysis, feature selection, values imputing}
\par \pard\plain \s0\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\sl256\slmult1\ql\widctlpar\faauto\sb0\sa160\hyphpar0\ltrpar\loch\f5\fs22\lang2057\cf0\kerning1\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa200\ltrpar{\rtlch \ltrch\lang1033\loch
The dataset presents ten tables, linked to each other with key columns. For making the dashboard more compact and reducing the execution time on the notebook, I will use the main table (application_train), the dependent one (bureau) and the one dependent from bureau (bureau_balance), just to illustrate the principle how these tables are joined one to another. The table application_train includes the TARGET column, which I should predict (1 means difficulties to pay, "positive target", "target 1", while 0 means all other cases, "negative target", "target 0"). The meaning of every variable of every table is available in the table "descriptions".}
\par \pard\plain \s0\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\sl256\slmult1\ql\widctlpar\faauto\sb0\sa160\hyphpar0\ltrpar\loch\f5\fs22\lang2057\cf0\kerning1\sl256\slmult1\li0\ri0\lin0\rin0\fi0\ltrpar{\rtlch\afs28\ab \ltrch\fs28\lang1033\b\loch
Application table}
\par \pard\plain \s0\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\sl256\slmult1\ql\widctlpar\faauto\sb0\sa160\hyphpar0\ltrpar\loch\f5\fs22\lang2057\cf0\kerning1\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa200\ltrpar{\rtlch \ltrch\lang1033\loch
The information about the client is filled to a high extent, while the information about }{\rtlch \ltrch\lang1033\loch
the client\u8217\'92s }{\rtlch \ltrch\lang1033\loch
dwelling is poorly filled. I worked separately with object data type, then with integers called "Flags", where you have 0 or 1 for each, or sometimes from 1 to 3 (like with RATING_REGION_CLIENT), then with integers that can take many values (like DAYS_BIRTH), then with floats. }
\par \pard\plain \s0\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\sl256\slmult1\ql\widctlpar\faauto\sb0\sa160\hyphpar0\ltrpar\loch\f5\fs22\lang2057\cf0\kerning1\sl256\slmult1\li0\ri0\lin0\rin0\fi0\ltrpar{\rtlch \ltrch\lang1033\loch
For treatment of object and flag variables, I created "find_most_risky" function, that for ever}{\rtlch \ltrch\lang1033\loch
y}{\rtlch \ltrch\lang1033\loch
 value of the variable shows }{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025 \ltrch\fs22\lang1033\loch
the percent of}{\rtlch \ltrch\lang1033\loch
 people}{\rtlch \ltrch\lang1033\loch
 }{\rtlch \ltrch\lang1033\loch
bearing this value }{\rtlch \ltrch\lang1033\loch
who }{\rtlch \ltrch\lang1033\loch
fail their credits. I created the function "category_to_fail_proportion" to replace categorical values with their probabilities of fail (see "limits and improvements" section). When variables looked correlated (like RATING_REGION_CLIENT and RATING_REGION_CLIENT with city), I explored them with confusion matrix, and see the fail probabilities with "find_most_risky" for the lines where these values differ (less than 1% of lines) to select which of them is more useful for the fail detection.}
\par \pard\plain \s0\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\sl256\slmult1\ql\widctlpar\faauto\sb0\sa160\hyphpar0\ltrpar\loch\f5\fs22\lang2057\cf0\kerning1\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa200\ltrpar{\rtlch \ltrch\lang1033\loch
Integer and float variables were explored with }{\rtlch \ltrch\lang1033\loch
histograms. }{\rtlch \ltrch\lang1033\loch
If the distribution is quasi-exponential, I apply log transform. In most cases, }{\rtlch \ltrch\lang1033\loch
after log transform}{\rtlch \ltrch\lang1033\loch
 it looks close to the normal distribution (for instance, INCOME_TOTAL or AMOUNT_ANNUITY), but sometimes distribution stays skewed (like TOTALAREA_MODE}{\rtlch \ltrch\lang1033\loch
\u8212\'97}{\rtlch \ltrch\lang1033\loch
maybe because it only looks like exponential, but in fact it's not).  }{\rtlch \ltrch\lang1033\loch
Then I used }{\rtlch \ltrch\lang1033\loch
kdeplots, }{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025 \ltrch\fs22\lang1033\loch
that}{\rtlch \ltrch\lang1033\loch
 show the probability of a client to have this value. I displayed the curves of client }{\rtlch \ltrch\lang1033\loch
groups}{\rtlch \ltrch\lang1033\loch
 with targets 0 and 1 on the same plot for comparison. }{\rtlch \ltrch\lang1033\loch
A}{\rtlch \ltrch\lang1033\loch
s the class distribution is imbalanced, I declined the common norm (so the distribution is within the group}{\rtlch \ltrch\lang1033\loch
0 and group1 independently}{\rtlch \ltrch\lang1033\loch
). When I see the difference in shape between curve0 and curve1, I select this variable. }{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025 \ltrch\fs22\lang1033\loch
S}{\rtlch \ltrch\lang1033\loch
urprisingly, INCOME_TOTAL has no impact on the curve shape, neither any synthetic variables derived from them. I tried some synthetic variables that looked relevant to me (ratios var1/var2) and their inverse}{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025 \ltrch\fs22\lang1033\loch
s}{\rtlch \ltrch\lang1033\loch
 (var2/var1). For most of them the derived synthetic variables show no difference for curve0 and curve1, except PAYMENT_RATE and CREDIT_TERM. The variables CNT_ADULTS shows the distinction on age distribution: there is a set of lonely old people who }{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025 \ltrch\fs22\lang1033\loch
manage to}{\rtlch \ltrch\lang1033\loch
 pay their credits. }{\rtlch \ltrch\lang1033\loch
F}{\rtlch \ltrch\lang1033\loch
rom the columns GENDER and FAMILY_STATUS I guess it's about widows who relatively rarely fail to pay their credits.}
\par \pard\plain \s0\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\sl256\slmult1\ql\widctlpar\faauto\sb0\sa160\hyphpar0\ltrpar\loch\f5\fs22\lang2057\cf0\kerning1\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa200\ltrpar{\rtlch \ltrch\lang1033\loch
Variables AMT_ANNUITY and AMT_CREDIT showed the biggest impact on the outcome of the model, as I see from shap summary_plot. }{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025 \ltrch\fs22\lang1033\loch
But}{\rtlch \ltrch\lang1033\loch
 these impacts are always the similar amplitude/abs value but in opposite directions. I suggest that's due to the fact they are negatively correlated. When I delete one of them, the other loses its high importance as well.}
\par \pard\plain \s0\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\sl256\slmult1\ql\widctlpar\faauto\sb0\sa160\hyphpar0\ltrpar\loch\f5\fs22\lang2057\cf0\kerning1\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa200\ltrpar{\rtlch \ltrch\lang1033\loch
The variables EXT_SOURCE_1, 2 and 3 are the most useful, but 1 and 3 have many empty cells. To maintain the relevance as much as I could, I check the probability of fail in missing values: it's merely the same as for the filled cells. So, I imputed the values from the crossing of target0 curve and target1 curve.}
\par \pard\plain \s0\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\sl256\slmult1\ql\widctlpar\faauto\sb0\sa160\hyphpar0\ltrpar\loch\f5\fs22\lang2057\cf0\kerning1\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa200\ltrpar{\rtlch \ltrch\lang1033\loch
The table contains many variables about client dwelling, I suppose aggregates from another table, as they have marks AVG, MODE and MEDI, and plenty of empty values. For every variable the kde curves are almost identical between var_AVG, var_MODE and var_MEDI. So, I selected only those who had different curves for target0 and target1 (with AVG aggregation) and dropped the rest 40 variables.}
\par \pard\plain \s0\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\sl256\slmult1\ql\widctlpar\faauto\sb0\sa160\hyphpar0\ltrpar\loch\f5\fs22\lang2057\cf0\kerning1\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa200\ltrpar{\rtlch\afs28\ab \ltrch\fs28\lang1033\b\loch
The dependent tables: bureau balance and bureau}
\par \pard\plain \s0\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\sl256\slmult1\ql\widctlpar\faauto\sb0\sa160\hyphpar0\ltrpar\loch\f5\fs22\lang2057\cf0\kerning1\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa200\ltrpar{\rtlch \ltrch\lang1033\loch
Table bureau balance doesn't have missing values. I aggregated the balance by count (how many months the client holds an account), STATUS by get dummies (this variable is important because it shows if the client had DPD (days past due) in his credit history. When I join the bureau_balance to bureau on the given key, bureau table has some missing values. I filled the dummies with X (which is unknown status) and the balance with 0.}
\par \pard\plain \s0\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\sl256\slmult1\ql\widctlpar\faauto\sb0\sa160\hyphpar0\ltrpar\loch\f5\fs22\lang2057\cf0\kerning1\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa200\ltrpar{\rtlch \ltrch\lang1033\loch
Bureau table has missing values in eight columns. To find out the best way to impute, I checked for each variable, how the lines where the variable has a missing value }{\rtlch \ltrch\lang1033\loch
look}{\rtlch \ltrch\lang1033\loch
. }{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025 \ltrch\fs22\lang1033\loch
T}{\rtlch \ltrch\lang1033\loch
he proportion of the bad debt statues is}{\rtlch \ltrch\lang1033\loch
n\u8217\'92t}{\rtlch \ltrch\lang1033\loch
 higher }{\rtlch \ltrch\lang1033\loch
for samples with missing variables}{\rtlch \ltrch\lang1033\loch
. I also looked at the lined having CREDIT_ACTIVE variable bad_debt value (there are only 21). Half of them has the debt sum 0 or overdue days 0. So, these variables aren't a good detector }{\rtlch \ltrch\lang1033\loch
of a bad_debt}{\rtlch \ltrch\lang1033\loch
. I filled }{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025 \ltrch\fs22\lang1033\loch
empty cells}{\rtlch \ltrch\lang1033\loch
 with 0, because I aggregate these columns by max or by sum. If I aggregate anything by mean, I will replace the missing values my mean value, in order not to shift the mean }{\rtlch \ltrch\lang1033\loch
to 0}{\rtlch \ltrch\lang1033\loch
. When I joined the bureau to application_train on the given key, quite many values are missing. I filled the numeric columns with the median (because I'm not going to aggregate anymore and want to have a "neutral" value that doesn't shift the decision). Categorial }{\rtlch \ltrch\lang1033\loch
variables were }{\rtlch \ltrch\lang1033\loch
filled }{\rtlch \ltrch\lang1033\loch
with 1 for}{\rtlch \ltrch\lang1033\loch
 "unknown", }{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025 \ltrch\fs22\lang1033\loch
and}{\rtlch \ltrch\lang1033\loch
 with 0 }{\rtlch \ltrch\lang1033\loch
for the rest}{\rtlch \ltrch\lang1033\loch
.}
\par \pard\plain \s0\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\sl256\slmult1\ql\widctlpar\faauto\sb0\sa160\hyphpar0\ltrpar\loch\f5\fs22\lang2057\cf0\kerning1\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa200\ltrpar{\rtlch \ltrch\lang1033\loch
Finally, I selected features in some iterations: I compared the models on full set of variables, I saw which variables have high shap values and kept them (except if they are correlated). I tried to drop the variables that were not in the list and check whether it reduces performance of the model. It turns out, many features can be dropped without harm (like CNT_FAM_MEMBERS). Some are essential (like EXT_SOURCE), and some are }{\rtlch \ltrch\lang1033\loch
of }{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025 \ltrch\fs22\lang1033\loch
smaller}{\rtlch \ltrch\lang1033\loch
 importan}{\rtlch \ltrch\lang1033\loch
ce}{\rtlch \ltrch\lang1033\loch
 (}{\rtlch \ltrch\lang1033\loch
they }{\rtlch \ltrch\lang1033\loch
reduce the performance }{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025 \ltrch\fs22\lang1033\loch
by}{\rtlch \ltrch\lang1033\loch
 0.5%).}
\par \pard\plain \s0\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\sl256\slmult1\ql\widctlpar\faauto\sb0\sa160\hyphpar0\ltrpar\loch\f5\fs22\lang2057\cf0\kerning1\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa200\ltrpar{\rtlch\afs32\ab \ltrch\fs32\lang1033\b\loch
Imbalanced class handling, hyperparameters adjustment}{\rtlch\afs22\ab \ltrch\fs22\lang1033\b\loch
 }
\par \pard\plain \s0\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\sl256\slmult1\ql\widctlpar\faauto\sb0\sa160\hyphpar0\ltrpar\loch\f5\fs22\lang2057\cf0\kerning1\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa200\ltrpar{\rtlch \ltrch\lang1033\loch
The class distribution is highly imbalanced: }{\rtlch \ltrch\lang1033\loch
only }{\rtlch \ltrch\lang1033\loch
8% of target }{\rtlch \ltrch\lang1033\loch
1}{\rtlch \ltrch\lang1033\loch
. That's why I used stratified split by target, }{\rtlch \ltrch\lang1033\loch
keeping}{\rtlch \ltrch\lang1033\loch
 20% }{\rtlch \ltrch\lang1033\loch
of samples}{\rtlch \ltrch\lang1033\loch
 }{\rtlch \ltrch\lang1033\loch
as the}{\rtlch \ltrch\lang1033\loch
 test }{\rtlch \ltrch\lang1033\loch
split}{\rtlch \ltrch\lang1033\loch
. }{\rtlch \ltrch\lang1033\loch
C}{\rtlch \ltrch\lang1033\loch
lass imbalance }{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025 \ltrch\fs22\lang1033\loch
is}{\rtlch \ltrch\lang1033\loch
 an issue during }{\rtlch \ltrch\lang1033\loch
the }{\rtlch \ltrch\lang1033\loch
model training }{\rtlch \ltrch\lang1033\loch
as well}{\rtlch \ltrch\lang1033\loch
. You can }{\rtlch \ltrch\lang1033\loch
s}{\rtlch \ltrch\lang1033\loch
olve }{\rtlch \ltrch\lang1033\loch
it}{\rtlch \ltrch\lang1033\loch
 by setting the \u8220\'93class weight\u8221\'94 parameter }{\rtlch \ltrch\lang1033\loch
to }{\rtlch \ltrch\lang1033\loch
"balanced", }{\rtlch \ltrch\lang1033\loch
that}{\rtlch \ltrch\lang1033\loch
 assigns the higher weights to the minor class samples. }{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025 \ltrch\fs22\lang1033\loch
A}{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025 \ltrch\fs22\lang1033\loch
lso}{\rtlch \ltrch\lang1033\loch
 you can use }{\rtlch \ltrch\lang1033\loch
an }{\rtlch \ltrch\lang1033\loch
oversampling technique, abbreviated as SMOTE. }{\rtlch \ltrch\lang1033\loch
It}{\rtlch \ltrch\lang1033\loch
 creates virtual samples of minor class with the values of these new samples }{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025 \ltrch\fs22\lang1033\loch
located on}{\rtlch \ltrch\lang1033\loch
 the vectors connecting any chosen sample with random samples of the same class.}
\par \pard\plain \s0\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\sl256\slmult1\ql\widctlpar\faauto\sb0\sa160\hyphpar0\ltrpar\loch\f5\fs22\lang2057\cf0\kerning1\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa200\ltrpar{\rtlch \ltrch\lang1033\loch
I compared 3 models: LogisticRegression, PassiveAggressiveClassifier and RandomForestClassifier. Logistic regression is a classic }{\rtlch \ltrch\lang1033\loch
model}{\rtlch \ltrch\lang1033\loch
, PassiveAggressiveClassifier is known to work as good as LogisticRegression but is known to be better in eliminating redundant features, and RandomForest is known to be }{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025 \ltrch\fs22\lang1033\loch
an}{\rtlch \ltrch\lang1033\loch
 efficient }{\rtlch \ltrch\lang1033\loch
predictor}{\rtlch \ltrch\lang1033\loch
, though tends to overfit. I wanted to try also the LightGBM algorithm (see "limits and improvements").}
\par \pard\plain \s0\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\sl256\slmult1\ql\widctlpar\faauto\sb0\sa160\hyphpar0\ltrpar\loch\f5\fs22\lang2057\cf0\kerning1\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa200\ltrpar{\rtlch \ltrch\lang1033\loch
I selected hyperparameters with exhaustive GridSearch. As }{\rtlch \ltrch\lang1033\loch
a full }{\rtlch \ltrch\lang1033\loch
grid-search }{\rtlch \ltrch\lang1033\loch
on all parameters is time-consuming}{\rtlch \ltrch\lang1033\loch
, I took only 1 or 2 key parameters, which }{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025 \ltrch\fs22\lang1033\loch
typically}{\rtlch \ltrch\lang1033\loch
 have the highest impact on model performance. }{\rtlch \ltrch\lang1033\loch
I used }{\rtlch \ltrch\lang1033\loch
ROC_AUC (}{\rtlch \ltrch\lang1033\loch
area under ROC curve}{\rtlch \ltrch\lang1033\loch
) }{\rtlch \ltrch\lang1033\loch
metric }{\rtlch \ltrch\lang1033\loch
for hyperparameters selection}{\rtlch \ltrch\lang1033\loch
, }{\rtlch \ltrch\lang1033\loch
because it is more representative than accuracy for the imbalanced class distributions}{\rtlch \ltrch\lang1033\loch
. }{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025 \ltrch\fs22\lang1033\loch
I }{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025 \ltrch\fs22\lang1033\loch
compared train }{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025 \ltrch\fs22\lang1033\loch
scores }{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025 \ltrch\fs22\lang1033\loch
and test scores from cross-validation t}{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025 \ltrch\fs22\lang1033\loch
o}{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025 \ltrch\fs22\lang1033\loch
 a check if the model t}{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025 \ltrch\fs22\lang1033\loch
ends to overfit}{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025 \ltrch\fs22\lang1033\loch
. }{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025 \ltrch\fs22\lang1033\loch
The randomForestClassifier shows much higher score on train set than on test set, so it\u8217\'92s overfitted, even with parameters preventing overfitting (}{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025 \ltrch\fs22\lang1033\loch
low number_estimators, }{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025 \ltrch\fs22\lang1033\loch
low max_depth, high min_impurity_decrease).}
\par \pard\plain \s0\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\sl256\slmult1\ql\widctlpar\faauto\sb0\sa160\hyphpar0\ltrpar\loch\f5\fs22\lang2057\cf0\kerning1\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa200\ltrpar{\rtlch\dbch\af7\langfe1036\dbch\af10\afs32\alang1025\ab \ltrch\fs32\lang1033\b\loch
Models comparison, personalized metric}
\par \pard\plain \s0\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\sl256\slmult1\ql\widctlpar\faauto\sb0\sa160\hyphpar0\ltrpar\loch\f5\fs22\lang2057\cf0\kerning1\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa200\ltrpar{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
Results of three tested models }{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
with 2 tested}{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
 methods of class }{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
re-balancing}{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
 were collected in a DataFrame and sorted with ROC AUC. }{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
LogisticRegression with class_weight=\u8220\'93balanced\u8221\'94 showed the highest score, so I used it for further step: personalized metric. }
\par \pard\plain \s0\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\sl256\slmult1\ql\widctlpar\faauto\sb0\sa160\hyphpar0\ltrpar\loch\f5\fs22\lang2057\cf0\kerning1\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa200\ltrpar{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
The metric that I chose is a cost function. A false negative prediction cost}{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
s}{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
 some money to the bank }{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
(a bad debt)}{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
. A false positive prediction cost}{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
s}{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
 some money }{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
too }{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
(}{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
bank }{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
lose}{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
s}{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
 a client). }{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
We can assign the costs as coefficients, and look to minimize this cost. For example, if a FN makes lose 10 times more money than a FP, we look where is the minimal loss 10* FN + FP. I applied this metric for looking the best hyperparameter }{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
C with the cross-validation. I}{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
 wanted to apply it to find the best probability threshold (see "limits and improvements"). }{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
Once the C is found, I trained the model on the full train set.}
\par \pard\plain \s0\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\sl256\slmult1\ql\widctlpar\faauto\sb0\sa160\hyphpar0\ltrpar\loch\f5\fs22\lang2057\cf0\kerning1\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa200\ltrpar{\rtlch\dbch\af7\langfe1036\dbch\af10\afs32\alang1025\ab \ltrch\fs32\lang1033\b\loch
Local feture importances (shap explainer)}
\par \pard\plain \s0\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\sl256\slmult1\ql\widctlpar\faauto\sb0\sa160\hyphpar0\ltrpar\loch\f5\fs22\lang2057\cf0\kerning1\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa200\ltrpar{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
An explainer is an object that learns on a set of samples, treating the model as a black box, and testing what is the outcome when it replaces this or that feature, one by one. }{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
The}{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
n it}{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
 }{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
explainer provides an expected value (somewhat like }{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
an }{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
intercept in a logistic regression) and a set of Shapley values }{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
(somewhat like coefficients in a linear regression)}{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
. Addition of a sample values multiplied by shapley values gives the shap prediction. The Shap library allows to make the graphs for }{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
one client or for a set of clients (see \u8220\'93limits and improvements\u8221\'94). I tested KernelExplainer, that is recommended as a universal for all kinds of models, and LinearExplainer. The latter has a drawback that can\u8217\'92t be overcome: it returns the prediction values outside of probability interval [0, 1].}
\par \pard\plain \s0\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\sl256\slmult1\ql\widctlpar\faauto\sb0\sa160\hyphpar0\ltrpar\loch\f5\fs22\lang2057\cf0\kerning1\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa200\ltrpar{\rtlch\dbch\af7\langfe1036\dbch\af10\afs32\alang1025\ab \ltrch\fs32\lang1033\b\loch
API and dashboard}
\par \pard\plain \s0\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\sl256\slmult1\ql\widctlpar\faauto\sb0\sa160\hyphpar0\ltrpar\loch\f5\fs22\lang2057\cf0\kerning1\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa200\ltrpar{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
API (Application programming interface) means a program that you launch on the computer through the command prompt. You send the requests to this program through the command prompt and receive the returns of the program. }{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
In our case, the program is the saved model. I launched the model first on my computer, before launching them on a server. Heroku is a free server, but it provides only one host per account. So, in order to make two accounts, I had to run two accounts in parallel on two notebooks.}
\par \pard\plain \s0\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\sl256\slmult1\ql\widctlpar\faauto\sb0\sa160\hyphpar0\ltrpar\loch\f5\fs22\lang2057\cf0\kerning1\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa200\ltrpar{\rtlch\dbch\af7\langfe1036\dbch\af10\afs32\alang1025\ab \ltrch\fs32\lang1033\b\loch
Limits and improvements}
\par \pard\plain \s0\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\sl256\slmult1\ql\widctlpar\faauto\sb0\sa160\hyphpar0\ltrpar\loch\f5\fs22\lang2057\cf0\kerning1\sl256\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa200\ltrpar{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
created the function "category_to_fail_proportion" to replace categorical values with their probabilities of fail (see "limits and improvements" section). }
\par \pard\plain \s0\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\sl256\slmult1\ql\widctlpar\faauto\sb0\sa160\hyphpar0\ltrpar\loch\f5\fs22\lang2057\cf0\kerning1\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa200\ltrpar{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab \ltrch\fs22\lang1033\b\loch
SMOTE}{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
 is }{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
claimed}{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
 to }{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
perform}{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
 better }{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
when followed by}{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
 }{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab \ltrch\fs22\lang1033\b\loch
undersampling techniques}{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
, such as Tomek Links or ENN. }{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
Including this step into the pipeline would probably improve the performance.}
\par \pard\plain \s0\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\sl256\slmult1\ql\widctlpar\faauto\sb0\sa160\hyphpar0\ltrpar\loch\f5\fs22\lang2057\cf0\kerning1\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa200\ltrpar{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab \ltrch\fs22\lang1033\b\loch
LightGBM}{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
, }{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
which is close to XGBoost by its principle of work,}{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
 }{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
is an interesting }{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
algorithm }{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
to try at the }{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab \ltrch\fs22\lang1033\b\loch
model comparison step}{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
. But it isn\u8217\'92t created in sklearn library, so it has different methods than sklearn algorithms (say, train instead of fit). And unlike the XGBoost, it has no sklearn wrapper. One possible workaround is to train and test a LIghtGBM algorithm outside the pipeline, to see its performance. But if the performance is ok, the limitation is to save the model. The overcome would be either write an sklearn wrapper for LightGBM, as it is done for XGBoost, or }{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
save the lightGBM model with joblib library without using sklearn pipeline and learn to load the saved model.}
\par \pard\plain \s0\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\sl256\slmult1\ql\widctlpar\faauto\sb0\sa160\hyphpar0\ltrpar\loch\f5\fs22\lang2057\cf0\kerning1\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa200\ltrpar{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
As }{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
a full }{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
grid-search }{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
on all parameters is time-consuming}{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
, I took only 1 or 2 key parameters, which }{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
typically}{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
 have the highest impact on model performance (see "limits and improvements" for other grid methods for hyperparameter choice and for stratified KFold for sampling during cross-validation). }
\par \pard\plain \s0\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\sl256\slmult1\ql\widctlpar\faauto\sb0\sa160\hyphpar0\ltrpar\loch\f5\fs22\lang2057\cf0\kerning1\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa200\ltrpar{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
I}{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
 wanted to apply }{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
the personalized metric }{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
to find the best probability threshold (see "limits and improvements"). }
\par \pard\plain \s0\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\sl256\slmult1\ql\widctlpar\faauto\sb0\sa160\hyphpar0\ltrpar\loch\f5\fs22\lang2057\cf0\kerning1\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa200\ltrpar{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
The Shap library allows to make the graphs for }{\rtlch\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\ab0 \ltrch\fs22\lang1033\b0\loch
one client or for a set of clients (see \u8220\'93limits and improvements\u8221\'94: show a client on a background of similar clients). }
\par \pard\plain \s0\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\sl256\slmult1\ql\widctlpar\faauto\sb0\sa160\hyphpar0\ltrpar\loch\f5\fs22\lang2057\cf0\kerning1\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa200\ltrpar\loch

\par \pard\plain \s0\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\sl256\slmult1\ql\widctlpar\faauto\sb0\sa160\hyphpar0\ltrpar\loch\f5\fs22\lang2057\cf0\kerning1\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa200\ltrpar\rtlch \ltrch\lang1033\loch

\par \pard\plain \s0\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\sl256\slmult1\ql\widctlpar\faauto\sb0\sa160\hyphpar0\ltrpar\loch\f5\fs22\lang2057\cf0\kerning1\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa200\ltrpar\rtlch \ltrch\lang1033\loch

\par \pard\plain \s0\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\sl256\slmult1\ql\widctlpar\faauto\sb0\sa160\hyphpar0\ltrpar\loch\f5\fs22\lang2057\cf0\kerning1\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa200\ltrpar\rtlch \ltrch\lang1033\loch

\par \pard\plain \s0\dbch\af7\langfe1036\dbch\af10\afs22\alang1025\sl256\slmult1\ql\widctlpar\faauto\sb0\sa160\hyphpar0\ltrpar\loch\f5\fs22\lang2057\cf0\kerning1\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa200\ltrpar\loch

\par }