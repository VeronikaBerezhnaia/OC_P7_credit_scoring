{\rtf1\ansi\deff3\adeflang1025
{\fonttbl{\f0\froman\fprq2\fcharset0 Times New Roman;}{\f1\froman\fprq2\fcharset2 Symbol;}{\f2\fswiss\fprq2\fcharset0 Arial;}{\f3\froman\fprq2\fcharset0 Liberation Serif{\*\falt Times New Roman};}{\f4\froman\fprq2\fcharset0 Calibri;}{\f5\froman\fprq2\fcharset0 Liberation Sans{\*\falt Arial};}{\f6\fnil\fprq2\fcharset0 Times New Roman;}{\f7\fswiss\fprq0\fcharset128 Lucida Sans;}{\f8\fnil\fprq2\fcharset0 Liberation Serif{\*\falt Times New Roman};}}
{\colortbl;\red0\green0\blue0;\red0\green0\blue255;\red0\green255\blue255;\red0\green255\blue0;\red255\green0\blue255;\red255\green0\blue0;\red255\green255\blue0;\red255\green255\blue255;\red0\green0\blue128;\red0\green128\blue128;\red0\green128\blue0;\red128\green0\blue128;\red128\green0\blue0;\red128\green128\blue0;\red128\green128\blue128;\red192\green192\blue192;}
{\stylesheet{\s0\snext0\dbch\af6\langfe1036\dbch\af8\afs22\alang1025\sl254\slmult1\ql\widctlpar\hyphpar0\faauto\sb0\sa160\ltrpar\loch\f4\fs22\lang2057\kerning1\cf0 Normal;}
{\*\cs15\snext15 Default Paragraph Font;}
{\*\cs16\sbasedon15\snext16\dbch\af6\afs16\fs16 annotation reference;}
{\*\cs17\sbasedon15\snext17\dbch\af6\afs20\fs20\lang1033 Comment Text Char;}
{\*\cs18\sbasedon17\snext18\dbch\af6\afs20\ab\fs20\lang1033\b Comment Subject Char;}
{\*\cs19\sbasedon15\snext19\afs20\fs20 Comment Text Char1;}
{\*\cs20\sbasedon19\snext20\afs20\ab\fs20\b Comment Subject Char1;}
{\s39\sbasedon0\snext40\dbch\af6\langfe1036\dbch\af8\afs28\alang1025\sl254\slmult1\ql\widctlpar\hyphpar0\faauto\sb240\sa120\keepn\loch\f5\fs28\lang2057\kerning1 Heading;}
{\s40\sbasedon0\snext40\dbch\af6\langfe1036\dbch\af8\afs22\alang1025\sl-276\slmult0\ql\widctlpar\hyphpar0\faauto\sb0\sa140\loch\f4\fs22\lang2057\kerning1 Text Body;}
{\s41\sbasedon40\snext41\dbch\af6\langfe1036\dbch\af8\afs22\alang1025\sl-276\slmult0\ql\widctlpar\hyphpar0\faauto\sb0\sa140\loch\f4\fs22\lang2057\kerning1 List;}
{\s42\sbasedon0\snext42\dbch\af6\langfe1036\dbch\af7\afs24\alang1025\ai\sl254\slmult1\ql\widctlpar\hyphpar0\faauto\sb120\sa120\noline\loch\f4\fs24\lang2057\i\kerning1 Caption;}
{\s43\sbasedon0\snext43\dbch\af6\langfe1036\dbch\af8\afs22\alang1025\sl254\slmult1\ql\widctlpar\hyphpar0\faauto\sb0\sa160\loch\f4\fs22\lang2057\kerning1 Index;}
{\s44\snext44\dbch\af6\langfe1036\dbch\af8\afs22\alang1025\sl254\slmult1\ql\widctlpar\hyphpar0\faauto\sb0\sa160\ltrpar\loch\f4\fs22\lang1036\kerning1\cf0 DocumentMap;}
{\s45\sbasedon0\snext45\dbch\af6\langfe1036\dbch\af8\afs24\alang1025\ai\sl254\slmult1\ql\widctlpar\hyphpar0\faauto\sb120\sa120\loch\f4\fs24\lang2057\i\kerning1 caption;}
{\s46\sbasedon0\snext46\dbch\af6\langfe1036\dbch\af8\afs20\alang1025\sl240\slmult1\ql\widctlpar\hyphpar0\faauto\sb0\sa160\loch\f4\fs20\lang2057\kerning1 annotation text;}
{\s47\sbasedon46\snext47\dbch\af6\langfe1036\dbch\af8\afs20\alang1025\ab\sl240\slmult1\ql\widctlpar\hyphpar0\faauto\sb0\sa160\loch\f4\fs20\lang2057\b\kerning1 annotation subject;}
{\s48\snext48\dbch\af6\langfe1036\dbch\af8\afs22\alang1025\ql\widctlpar\hyphpar0\faauto\ltrpar\loch\f4\fs22\lang1033\kerning1\cf0 Revision;}
}{\*\generator LibreOffice/6.3.3.2$Windows_X86_64 LibreOffice_project/a64200df03143b798afd1ec74a12ab50359878ed}{\info{\creatim\yr2022\mo2\dy9\hr12\min57}{\revtim\yr2022\mo2\dy14\hr15\min43}{\printim\yr2022\mo2\dy13\hr22\min39}}{\*\userprops{\propname Operator}\proptype30{\staticval Morenkov, Oleg}}\deftab720\deftab720\deftab720
\hyphauto0\viewscale130
{\*\pgdsctbl
{\pgdsc0\pgdscuse451\pgwsxn12240\pghsxn15840\marglsxn1080\margrsxn1080\margtsxn1440\margbsxn1440\pgdscnxt0 Default Style;}}
\formshade{\*\pgdscno0}\paperh15840\paperw12240\margl1080\margr1080\margt1440\margb1440\sectd\sbknone\sectunlocked1\pgndec\pgwsxn12240\pghsxn15840\marglsxn1080\margrsxn1080\margtsxn1440\margbsxn1440\ftnbj\ftnstart1\ftnrstcont\ftnnar\aenddoc\aftnrstcont\aftnstart1\aftnnrlc
{\*\ftnsep\chftnsep}\pgndec\pard\plain \s0\dbch\af6\langfe1036\dbch\af8\afs22\alang1025\sl254\slmult1\ql\widctlpar\hyphpar0\faauto\sb0\sa160\ltrpar\loch\f4\fs22\lang2057\kerning1\cf0\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa120\ltrpar{\rtlch\afs32\ab \ltrch\fs32\lang1033\b\loch
Exploratory data analysis, feature selection, values imputing}
\par \pard\plain \s0\dbch\af6\langfe1036\dbch\af8\afs22\alang1025\sl254\slmult1\ql\widctlpar\hyphpar0\faauto\sb0\sa160\ltrpar\loch\f4\fs22\lang2057\kerning1\cf0\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa120\ltrpar{\rtlch\afs24 \ltrch\lang1033\loch
The dataset presents ten tables, linked to each other with key columns. For making the dashboard more compact and reducing the execution time on the notebook, I will use only the main table (application train), the dependent one (bureau) and the one dependent from bureau (bureau balance), in order to illustrate the principle how these tables are joined one to another. The table application_train includes the TARGET column (1 means difficulties to pay, "positive target", "target1", 0 means all other cases, "negative target", "target0"). The meaning of all variables of all tables is available in "descriptions" table.}
\par \pard\plain \s0\dbch\af6\langfe1036\dbch\af8\afs22\alang1025\sl254\slmult1\ql\widctlpar\hyphpar0\faauto\sb0\sa160\ltrpar\loch\f4\fs22\lang2057\kerning1\cf0\sl254\slmult1\li0\ri0\lin0\rin0\fi0\sb0\sa120\ltrpar{\rtlch\ab \ltrch\lang1033\b\loch
Application_train table}{\rtlch\ab \ltrch\b\loch
:}{\rtlch\afs24 \ltrch\lang1033\loch
 The information about the client is filled to a high extent, while the information about the client\u8217\'92s dwelling is poorly filled (probably it comes from aggregation and join from another table). I worked separately with object data type, then with integers called "Flags", where you have 0 or 1 for each (or flags from 1 to 3), then with integers that can take many values (like DAYS_BIRTH), then with floats. }
\par \pard\plain \s0\dbch\af6\langfe1036\dbch\af8\afs22\alang1025\sl254\slmult1\ql\widctlpar\hyphpar0\faauto\sb0\sa160\ltrpar\loch\f4\fs22\lang2057\kerning1\cf0\sl254\slmult1\li0\ri0\lin0\rin0\fi0\sb0\sa120\ltrpar{\rtlch\afs24 \ltrch\lang1033\loch
For treatment of object and flag variables, I created "find_most_risky" function, that shows }{\rtlch \ltrch\lang1033\loch
the percent of}{\rtlch\afs24 \ltrch\lang1033\loch
 people bearing every value of a given variable who fail their credits. I created the function "category_to_fail_proportion" to replace categorical values with their probabilities of fail (see "limits and improvements"). When variables looked correlated (like RATING_REGION_CLIENT and RATING_REGION_CLIENT with city), I explored them with confusion matrix, and see the fail probabilities with "find_most_risky" for the lines where these values differ (less than 1% of lines) to select which of them is more useful for the failure detection.}
\par \pard\plain \s0\dbch\af6\langfe1036\dbch\af8\afs22\alang1025\sl254\slmult1\ql\widctlpar\hyphpar0\faauto\sb0\sa160\ltrpar\loch\f4\fs22\lang2057\kerning1\cf0\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa120\ltrpar{\rtlch\afs24 \ltrch\lang1033\loch
Integer and float variables were explored with histograms. If the distribution is quasi-exponential, I apply log transform. In most cases, after log transform it looks close to the normal distribution (for instance, INCOME_TOTAL or AMOUNT_ANNUITY), but sometimes distribution stays skewed (like TOTALAREA_MODE\u8212\'97maybe because it only looks like exponential, but in fact it's not).  Then I used kdeplots, }{\rtlch \ltrch\lang1033\loch
that}{\rtlch\afs24 \ltrch\lang1033\loch
 show the probability of a client to have this value. I displayed the curves of client groups with targets 0 and 1 on the same plot for comparison. As the class distribution is imbalanced, I declined the common norm (so the distribution is within the group0 and group1 independently). When I see the difference in shape between curve0 and curve1, I select this variable. }{\rtlch \ltrch\lang1033\loch
S}{\rtlch\afs24 \ltrch\lang1033\loch
urprisingly, INCOME_TOTAL has no impact on the curve shape, neither any synthetic variables derived from it. I tried some synthetic variables that looked relevant to me (ratios var1/var2) and their inverse}{\rtlch \ltrch\lang1033\loch
s}{\rtlch\afs24 \ltrch\lang1033\loch
 (var2/var1). For most of them the derived synthetic variables show no difference for curve0 and curve1, except PAYMENT_RATE and CREDIT_TERM. The variables CNT_ADULTS shows the distinction on age distribution: there is a set of lonely old people who }{\rtlch \ltrch\lang1033\loch
manage to}{\rtlch\afs24 \ltrch\lang1033\loch
 pay their credits. From the columns GENDER and FAMILY_STATUS I guess it's about widows who relatively rarely fail to pay their credits.}
\par \pard\plain \s0\dbch\af6\langfe1036\dbch\af8\afs22\alang1025\sl254\slmult1\ql\widctlpar\hyphpar0\faauto\sb0\sa160\ltrpar\loch\f4\fs22\lang2057\kerning1\cf0\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa120\ltrpar{\rtlch\afs24 \ltrch\lang1033\loch
Variables AMT_ANNUITY and AMT_CREDIT showed the biggest impact on the outcome of the model, as I see from shap summary_plot. }{\rtlch \ltrch\lang1033\loch
But}{\rtlch\afs24 \ltrch\lang1033\loch
 these impacts are always the similar amplitude/abs value but in opposite directions. I suggest that's due to the fact they are negatively correlated. When I delete one of them, the other loses its high importance as well.}
\par \pard\plain \s0\dbch\af6\langfe1036\dbch\af8\afs22\alang1025\sl254\slmult1\ql\widctlpar\hyphpar0\faauto\sb0\sa160\ltrpar\loch\f4\fs22\lang2057\kerning1\cf0\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa120\ltrpar{\rtlch\afs24 \ltrch\lang1033\loch
The variables EXT_SOURCE_1, 2 and 3 are the most useful, but 1 and 3 have many empty cells. To maintain the relevance as much as I could, I check the probability of fail in missing values: it's merely the same as for the filled cells. So, I imputed the values from the crossing of target0 curve and target1 curve.}
\par \pard\plain \s0\dbch\af6\langfe1036\dbch\af8\afs22\alang1025\sl254\slmult1\ql\widctlpar\hyphpar0\faauto\sb0\sa160\ltrpar\loch\f4\fs22\lang2057\kerning1\cf0\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa120\ltrpar{\rtlch\afs24 \ltrch\lang1033\loch
The table contains many variables about client dwelling, I suppose aggregates from another table, as they have marks AVG, MODE and MEDI, and plenty of empty values. For every variable the kde curves are almost identical between var_AVG, var_MODE and var_MEDI. So, I selected only those who had different curves for target0 and target1 (with AVG aggregation) and dropped the rest 40 variables.}
\par \pard\plain \s0\dbch\af6\langfe1036\dbch\af8\afs22\alang1025\sl254\slmult1\ql\widctlpar\hyphpar0\faauto\sb0\sa160\ltrpar\loch\f4\fs22\lang2057\kerning1\cf0\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa120\ltrpar{\rtlch\ab \ltrch\lang1033\b\loch
The dependent tables: }{{\*\bkmkstart __DdeLink__460_323612619}\rtlch\ab \ltrch\lang1033\b\loch
bureau balance and bureau}{{\*\bkmkend __DdeLink__460_323612619}\rtlch\ab \ltrch\lang1033\b\loch
:}{\rtlch\afs24 \ltrch\loch
 }{\rtlch\afs24 \ltrch\lang1033\loch
table bureau balance doesn't have missing values. I aggregated the balance by count (how many months the client holds an account), STATUS by get dummies (this variable is important because it shows if the client had DPD (days past due) in his credit history. When I join the bureau_balance to bureau on the given key, bureau table has some missing values. I filled the dummies with X (which is unknown status) and the balance with 0.}
\par \pard\plain \s0\dbch\af6\langfe1036\dbch\af8\afs22\alang1025\sl254\slmult1\ql\widctlpar\hyphpar0\faauto\sb0\sa160\ltrpar\loch\f4\fs22\lang2057\kerning1\cf0\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa120\ltrpar{\rtlch\afs24 \ltrch\lang1033\loch
Bureau table has missing values in eight columns. To find out the best way to impute, I checked for each variable, how the lines where the variable has a missing value look. }{\rtlch \ltrch\lang1033\loch
T}{\rtlch\afs24 \ltrch\lang1033\loch
he proportion of the bad debt statues isn\u8217\'92t higher for samples with missing variables. I also looked at the lined having CREDIT_ACTIVE variable bad_debt value (there are only 21). Half of them has the debt sum 0 or overdue days 0. So, these variables aren't a good detector of a bad_debt. I filled }{\rtlch \ltrch\lang1033\loch
empty cells}{\rtlch\afs24 \ltrch\lang1033\loch
 with 0, because I aggregate these columns by max or by sum. If I aggregate anything by mean, I will replace the missing values my mean value, in order not to shift the mean to 0. When I joined the bureau to application_train on the given key, quite many values are missing. I filled the numeric columns with the median (because I'm not going to aggregate anymore and want to have a "neutral" value that doesn't shift the decision). Categorial variables were filled with 1 for "unknown", }{\rtlch \ltrch\lang1033\loch
and}{\rtlch\afs24 \ltrch\lang1033\loch
 with 0 for the rest.}
\par \pard\plain \s0\dbch\af6\langfe1036\dbch\af8\afs22\alang1025\sl254\slmult1\ql\widctlpar\hyphpar0\faauto\sb0\sa160\ltrpar\loch\f4\fs22\lang2057\kerning1\cf0\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa120\ltrpar{\rtlch\afs24 \ltrch\lang1033\loch
Finally, I selected features iteratively: I compared the models on full set of variables, I saw which variables have high shap values and kept them (except if they are correlated). I tried to drop the variables that were not in the list and check whether it reduces performance of the model. It turns out, many features can be dropped without harm (like CNT_FAM_MEMBERS). Some are essential (like EXT_SOURCE), and some are of }{\rtlch \ltrch\lang1033\loch
smaller}{\rtlch\afs24 \ltrch\lang1033\loch
 importance (they reduce the performance }{\rtlch \ltrch\lang1033\loch
by}{\rtlch\afs24 \ltrch\lang1033\loch
 0.5%).}
\par \pard\plain \s0\dbch\af6\langfe1036\dbch\af8\afs22\alang1025\sl254\slmult1\ql\widctlpar\hyphpar0\faauto\sb0\sa160\ltrpar\loch\f4\fs22\lang2057\kerning1\cf0\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa120\ltrpar{\rtlch\afs32\ab \ltrch\fs32\lang1033\b\loch
Model training methodology, optimization algorithm}
\par \pard\plain \s0\dbch\af6\langfe1036\dbch\af8\afs22\alang1025\sl254\slmult1\ql\widctlpar\hyphpar0\faauto\sb0\sa160\ltrpar\loch\f4\fs22\lang2057\kerning1\cf0\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa120\ltrpar{\rtlch\ab \ltrch\lang1033\b\loch
Imbalanced class handling: }{\rtlch\afs24 \ltrch\lang1033\loch
The class distribution is highly imbalanced: only 8% of target 1. That's why I used stratified split by target, keeping 20% of samples as the test split. Class imbalance }{\rtlch \ltrch\lang1033\loch
is}{\rtlch\afs24 \ltrch\lang1033\loch
 an issue during the model training as well. You can solve it by setting the \u8220\'93class weight\u8221\'94 parameter to "balanced", that assigns the higher weights to the minor class samples. }{\rtlch \ltrch\lang1033\loch
Also,}{\rtlch\afs24 \ltrch\lang1033\loch
 you can use an oversampling technique, abbreviated as SMOTE. It creates virtual samples of minor class with the values of these new samples }{\rtlch \ltrch\lang1033\loch
located on}{\rtlch\afs24 \ltrch\lang1033\loch
 the vectors connecting any chosen sample with random samples of the same class.}
\par \pard\plain \s0\dbch\af6\langfe1036\dbch\af8\afs22\alang1025\sl254\slmult1\ql\widctlpar\hyphpar0\faauto\sb0\sa160\ltrpar\loch\f4\fs22\lang2057\kerning1\cf0\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa120\ltrpar{\rtlch\afs24\ab \ltrch\lang1033\b\loch
Model choice:}{\rtlch\afs24 \ltrch\lang1033\loch
 I compared 5 models: Logistic Regression, Passive Aggressive Classifier, Random Forest Classifier. Logistic regression is a classic model, Passive Aggressive Classifier is known to work as good as Logistic Regression but is known to work faster and better eliminate redundant features, RandomForest is known to be }{\rtlch \ltrch\lang1033\loch
an}{\rtlch\afs24 \ltrch\lang1033\loch
 efficient predictor, though tends to overfit. The XGBClassifier and LightGBM algorithm were trained only with SMOTE class re-balancing, because these algorithms don\u8217\'92t have the class_weight parameter that I could set to \u8220\'93balanced\u8221\'94 (see "limits and improvements").}
\par \pard\plain \s0\dbch\af6\langfe1036\dbch\af8\afs22\alang1025\sl254\slmult1\ql\widctlpar\hyphpar0\faauto\sb0\sa160\ltrpar\loch\f4\fs22\lang2057\kerning1\cf0\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa120\ltrpar{\rtlch\ab \ltrch\lang1033\b\loch
Hyperparameters adjustment: }{\rtlch\afs24 \ltrch\lang1033\loch
I selected hyperparameters with exhaustive GridSearch. As a full grid-search on all parameters is time-consuming, I took only 1 or 2 key parameters, which }{\rtlch \ltrch\lang1033\loch
typically}{\rtlch\afs24 \ltrch\lang1033\loch
 have the highest impact on model performance. }{\rtlch \ltrch\lang1033\loch
I compared train scores and test scores from cross-validation to a check if the model tends to overfit. The Random Forest Classifier, XGBoost and LightGBM show much higher score on train set than on test set, so it\u8217\'92s overfitted, even with parameters preventing overfitting (for Random Forest Classifier - low number_estimators, low max_depth, high min_impurity_decrease).}
\par \pard\plain \s0\dbch\af6\langfe1036\dbch\af8\afs22\alang1025\sl254\slmult1\ql\widctlpar\hyphpar0\faauto\sb0\sa160\ltrpar\loch\f4\fs22\lang2057\kerning1\cf0\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa120\ltrpar{\rtlch\afs32\ab \ltrch\fs32\lang1033\b\loch
The evaluation metric and the business cost function}
\par \pard\plain \s0\dbch\af6\langfe1036\dbch\af8\afs22\alang1025\sl254\slmult1\ql\widctlpar\hyphpar0\faauto\sb0\sa160\ltrpar\loch\f4\fs22\lang2057\kerning1\cf0\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa120\ltrpar{\rtlch\afs24\ab \ltrch\lang1033\b\loch
The evaluation metric: }{\rtlch\afs24 \ltrch\lang1033\loch
I used ROC_AUC (area under ROC curve) metric for hyperparameters selection, because it is more representative than accuracy for the imbalanced class distributions. }{\rtlch \ltrch\lang1033\loch
Results of three tested models with 2 tested methods of class re-balancing were collected in a DataFrame and sorted with ROC AUC. LogisticRegression with class_weight = \u8220\'93balanced\u8221\'94 showed the highest score, so I used it for further step: personalized metric. }
\par \pard\plain \s0\dbch\af6\langfe1036\dbch\af8\afs22\alang1025\sl254\slmult1\ql\widctlpar\hyphpar0\faauto\sb0\sa160\ltrpar\loch\f4\fs22\lang2057\kerning1\cf0\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa120\ltrpar{\rtlch\ab \ltrch\lang1033\b\loch
The business cost function:}{\rtlch \ltrch\lang1033\loch
 A false negative prediction costs some money to the bank (a bad debt). A false positive prediction costs some money too (bank loses a client). We can assign the costs as coefficients and look to minimize this cost. For example, if a FN makes lose 10 times more money than a FP, we look where is the minimal loss 10* FN + FP. I applied this metric for looking the best hyperparameter C with the cross-validation. I wanted to apply it to find the best probability threshold (see "limits and improvements"). Once the C is found, I trained the model on the full train set.}
\par \pard\plain \s0\dbch\af6\langfe1036\dbch\af8\afs22\alang1025\sl254\slmult1\ql\widctlpar\hyphpar0\faauto\sb0\sa160\ltrpar\loch\f4\fs22\lang2057\kerning1\cf0\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa120\ltrpar{\rtlch\afs32\ab \ltrch\fs32\lang1033\b\loch
Local feture importances (shap explainer)}
\par \pard\plain \s0\dbch\af6\langfe1036\dbch\af8\afs22\alang1025\sl254\slmult1\ql\widctlpar\hyphpar0\faauto\sb0\sa160\ltrpar\loch\f4\fs22\lang2057\kerning1\cf0\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa120\ltrpar{\rtlch \ltrch\lang1033\loch
I used the feature explainer from shap library. An explainer is an object that learns on a set of samples, treating the model as a black box, and testing what is the outcome when it replaces this or that feature, one by one. Then the explainer provides an expected value (somewhat like the intercept in a linear regression) and a set of Shapley values (somewhat like the coefficients in a linear regression). Addition of a sample values multiplied by Shapley values gives the prediction of a shap explainer. The Shap library allows to make the graphs for one client or for a set of clients (see \u8220\'93limits and improvements\u8221\'94). I tested KernelExplainer, that is recommended as a universal for all kinds of models, and LinearExplainer. The latter has a drawback: it returns the prediction values outside of probability interval [0, 1].}
\par \pard\plain \s0\dbch\af6\langfe1036\dbch\af8\afs22\alang1025\sl254\slmult1\ql\widctlpar\hyphpar0\faauto\sb0\sa160\ltrpar\loch\f4\fs22\lang2057\kerning1\cf0\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa120\ltrpar{\rtlch\afs32\ab \ltrch\fs32\lang1033\b\loch
API and dashboard}
\par \pard\plain \s0\dbch\af6\langfe1036\dbch\af8\afs22\alang1025\sl254\slmult1\ql\widctlpar\hyphpar0\faauto\sb0\sa160\ltrpar\loch\f4\fs22\lang2057\kerning1\cf0\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa120\ltrpar{\rtlch \ltrch\lang1033\loch
API (Application programming interface) means a program that you launch on the computer through the command prompt. You send the requests to this program through the command prompt and receive the returns of the program. In our case, the program is the saved model. I launched the model first on my computer, before launching them on a server. Heroku is a free server, but it provides only one host per account. So, in order to make two accounts, I had to run two accounts in parallel on two notebooks.}
\par \pard\plain \s0\dbch\af6\langfe1036\dbch\af8\afs22\alang1025\sl254\slmult1\ql\widctlpar\hyphpar0\faauto\sb0\sa160\ltrpar\loch\f4\fs22\lang2057\kerning1\cf0\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa120\ltrpar{\rtlch\afs32\ab \ltrch\fs32\lang1033\b\loch
Limits and improvements}
\par \pard\plain \s0\dbch\af6\langfe1036\dbch\af8\afs22\alang1025\sl254\slmult1\ql\widctlpar\hyphpar0\faauto\sb0\sa160\ltrpar\loch\f4\fs22\lang2057\kerning1\cf0\sl254\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa120\ltrpar{\rtlch \ltrch\lang1033\loch
1. }{\rtlch\ab \ltrch\lang1033\b\loch
The function category_to_fail_proportion}{\rtlch \ltrch\lang1033\loch
 decreases the dimensionality of the table: one column instead of a dummy column for N values of a categorical variable. Worth comparing results with this function or without it. But the model can be saved only in a pipeline, and this transformation is done out of pipeline. To integrate this function into the pipeline, a new class \u8220\'93custom column transformer\u8221\'94 should be created that implements this function.}
\par \pard\plain \s0\dbch\af6\langfe1036\dbch\af8\afs22\alang1025\sl254\slmult1\ql\widctlpar\hyphpar0\faauto\sb0\sa160\ltrpar\loch\f4\fs22\lang2057\kerning1\cf0\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa120\ltrpar{\rtlch \ltrch\lang1033\loch
2.}{\rtlch\ab \ltrch\lang1033\b\loch
 }{\rtlch \ltrch\lang1033\loch
SMOTE is claimed to perform better when followed by }{\rtlch\ab \ltrch\lang1033\b\loch
undersampling techniques}{\rtlch \ltrch\lang1033\loch
, such as Tomek Links or ENN. Including this step into the pipeline would probably improve the performance. Also, the undersampling techniques are a must for the algorithms like XGBoost or lightGBM. LightGBM took 1h to be executed with empty parameters grid when using SMOTE. The XGBoost is expected to take even more time than LightGBM.}
\par \pard\plain \s0\dbch\af6\langfe1036\dbch\af8\afs22\alang1025\sl254\slmult1\ql\widctlpar\hyphpar0\faauto\sb0\sa160\ltrpar\loch\f4\fs22\lang2057\kerning1\cf0\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa120\ltrpar{\rtlch \ltrch\lang1033\loch
3.}{\rtlch\ab \ltrch\lang1033\b\loch
 LightGBM}{\rtlch \ltrch\lang1033\loch
, which is close to XGBoost by its principle of work, is an interesting algorithm to try at the }{\rtlch\ab \ltrch\lang1033\b\loch
model comparison step}{\rtlch \ltrch\lang1033\loch
. But it isn\u8217\'92t created in sklearn library, so it has different methods than sklearn algorithms (say, train instead of fit). And unlike the XGBoost, it has no sklearn wrapper. One possible workaround is to train and test a LIghtGBM algorithm outside the pipeline, to see its performance. But if the performance is ok, the limitation is to save the model. The overcome would be either write a sklearn wrapper for LightGBM, as it is done for XGBoost, or save the lightGBM model with joblib library without using sklearn pipeline and learn to load the saved model.}
\par \pard\plain \s0\dbch\af6\langfe1036\dbch\af8\afs22\alang1025\sl254\slmult1\ql\widctlpar\hyphpar0\faauto\sb0\sa160\ltrpar\loch\f4\fs22\lang2057\kerning1\cf0\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa120\ltrpar{\rtlch \ltrch\lang1033\loch
4. Try }{\rtlch\ab \ltrch\lang1033\b\loch
other grid search strategies}{\rtlch \ltrch\lang1033\loch
 (like RandomizedSearchCV) in order to compare more parameters faster.}
\par \pard\plain \s0\dbch\af6\langfe1036\dbch\af8\afs22\alang1025\sl254\slmult1\ql\widctlpar\hyphpar0\faauto\sb0\sa160\ltrpar\loch\f4\fs22\lang2057\kerning1\cf0\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa120\ltrpar{\rtlch \ltrch\lang1033\loch
5. Make sure that folding in cross-validation is stratified (see the documentation: }{\rtlch\afs24 \ltrch\loch
When the cv argument is an integer, cross_val_score uses the KFold or StratifiedKFold strategies by default, the latter being used if the estimator derives from ClassifierMixin}{\rtlch\afs24 \ltrch\lang4108\loch
) and }{\rtlch\afs24\ab \ltrch\lang4108\b\loch
ensure the use StratifiedKFold in XGBoost or LIghtGBM}{\rtlch\dbch\af6\afs24 \ltrch\lang4108\loch
.}
\par \pard\plain \s0\dbch\af6\langfe1036\dbch\af8\afs22\alang1025\sl254\slmult1\ql\widctlpar\hyphpar0\faauto\sb0\sa160\ltrpar\loch\f4\fs22\lang2057\kerning1\cf0\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa120\ltrpar{\rtlch \ltrch\lang1033\loch
6. The probability threshold in pipeline \u8220\'93predict\u8221\'94 method for a binary classification is 0.5. It would be useful to select the }{\rtlch\ab \ltrch\lang1033\b\loch
threshold that provides the optimal value of the cost function}{\rtlch \ltrch\lang1033\loch
. One way to do that is to include the value of this threshold into the grid_search. For that the \u8220\'93predict\u8221\'94 method of the pipeline must be rewritten in a way that it returns the probability of belonging to the class 1 instead of a class label. }
\par \pard\plain \s0\dbch\af6\langfe1036\dbch\af8\afs22\alang1025\sl254\slmult1\ql\widctlpar\hyphpar0\faauto\sb0\sa160\ltrpar\loch\f4\fs22\lang2057\kerning1\cf0\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa120\ltrpar{\rtlch\afs24 \ltrch\loch
7. To make group shap }{\rtlch\afs24\ab \ltrch\b\loch
plots that }{\rtlch\ab \ltrch\lang1033\b\loch
show a client on a background of similar clients}{\rtlch \ltrch\lang1033\loch
. For that it\u8217\'92s needed to define what exactly similar clients are. For example, having the same values \u8211\'96 of which variables? Or if these values are floats \u8211\'96 what\u8217\'92s the bin size? How to withdraw these similar clients from the dataset (if we don\u8217\'92t load the full dataset into the dashboard).}
\par \pard\plain \s0\dbch\af6\langfe1036\dbch\af8\afs22\alang1025\sl254\slmult1\ql\widctlpar\hyphpar0\faauto\sb0\sa160\ltrpar\loch\f4\fs22\lang2057\kerning1\cf0\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa120\ltrpar{\rtlch \ltrch\lang1033\loch
8. To retrieve data from the full dataset to the dashboard and to the API (maybe the dataset is stored in yet another computer than API or dashboard). }{\rtlch \ltrch\lang4108\loch
T}{\rtlch \ltrch\lang1033\loch
he command for the request must be learned, with all the parameters and specifications.}
\par \pard\plain \s0\dbch\af6\langfe1036\dbch\af8\afs22\alang1025\sl254\slmult1\ql\widctlpar\hyphpar0\faauto\sb0\sa160\ltrpar\loch\f4\fs22\lang2057\kerning1\cf0\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa120\ltrpar\rtlch\dbch\af6\afs24 \ltrch\lang1033\loch

\par \pard\plain \s0\dbch\af6\langfe1036\dbch\af8\afs22\alang1025\sl254\slmult1\ql\widctlpar\hyphpar0\faauto\sb0\sa160\ltrpar\loch\f4\fs22\lang2057\kerning1\cf0\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa120\ltrpar\rtlch\dbch\af6\afs24 \ltrch\lang1033\loch

\par \pard\plain \s0\dbch\af6\langfe1036\dbch\af8\afs22\alang1025\sl254\slmult1\ql\widctlpar\hyphpar0\faauto\sb0\sa160\ltrpar\loch\f4\fs22\lang2057\kerning1\cf0\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa120\ltrpar\rtlch\dbch\af6\afs24 \ltrch\lang1033\loch

\par \pard\plain \s0\dbch\af6\langfe1036\dbch\af8\afs22\alang1025\sl254\slmult1\ql\widctlpar\hyphpar0\faauto\sb0\sa160\ltrpar\loch\f4\fs22\lang2057\kerning1\cf0\sl276\slmult1\nowidctlpar\li0\ri0\lin0\rin0\fi0\sb0\sa120\ltrpar\loch

\par }