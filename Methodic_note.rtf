{\rtf1\ansi\ansicpg1252\deff0\nouicompat\deflang1033{\fonttbl{\f0\fnil\fcharset0 Calibri;}}
{\*\generator Riched20 10.0.19041}\viewkind4\uc1 
\pard\sa200\sl276\slmult1\b\f0\fs32\lang12 Exploratory data analysis, feature selection, values imputing\par
\b0\fs22 The dataset presents ten tables, linked to each other with key columns. For making the dashboard more compact and reducing the execution time on the notebook, I will use the main table (application_train) , the dependent one (bureau) and the one dependent from bureau (bureau_balance), just to illustrate the principle how these tables are joined one to another. Table application_train includes the TARGET column, which I should predict (1 is difficulties to pay, "positive target", "target 1", while 0 means all other cases "negative target", "target 0".\par
The meaning of every variable of every table is available in the table "descriptions".\par
\b\fs28 Application table\par
\b0\fs22 The information about the client is filled quite fully, while the information about dwelling is poorly filled. I worked separately with object data type, then integers called "Flags", where you have 0 or 1 for each, or sometimes from 1 to 3 (like with RATING_REGION_CLIENT), then integers that can take many values (like DAYS_BIRTH), then floats. \par
For treatment of object and flag variables, I created "find most risky" function, that for evert value of the variable shows how many people bearing this value fail their credits (in percent). I created the function "category_to_fail_proportion" to replace categorical values with their probabilities of fail (see "limits and improvements" section). When variables look correlated (like RATING_REGION_CLIENT and RATING_REGION_CLIENT with city), I explore them with confusion matrix, and see fail probabilities with "find most risky" for the applications where these values differ (less than 1% of applications) to select which is more useful for fail detection.\par
For integer and float variables I  explored them with kdeplots, that shows the probability of a client to have this value. I displayed the curves of clients set with targets 0 and 1 on the same plot for comparison, and as the classes are imbalancely distributed, I declined the common norm. When I see the difference in shape between curve0 and curve1, I select this variable. If the distribution is quasi-exponential, I apply log transform. In most cases it looks close to the normal distribution (for instance, INCOME_TOTAL or AMOUNT_ANNUITY), but sometimes distribution stays skewed (like TOTALAREA_MODE - maybe because it only looks like exponential, but in fact it's not). Thus, surplrisingly, INCOME_TOTAL has no impact on the curve shape, neither any synthetic variables derived from them. I tried some synthetic variables that looked relevant to me (ratios var1/var2) and their inverse just in case (var2/var1). For most of them they the derived synthetic variables show no difference for curve0 and curve1, except PAYMENT_RATE and CREDIT_TERM that I kept. The variables CNT_ADULTS shows the distinction on age distribution: there is a set of lonely old people who nicely pay their credits (from the columns GENDER and FAMILY_STATUS I guess it's about widows who relatively rarely fail to pay their credits).\par
Variables AMT_ANNUITY and AMT_CREDIT showed the biggest impact on the outcome of the model, as I see from shap summary_plot. Though these impacts are always the similar amplitude/abs value but in opposite directions. I suggest that's due to the fact they are negatively correlated. When I delete one fof them, the other losts its high importance as well.\par
The variables EXT_SOURCE_1, 2 and 3 are the most useful, but 1 and 3 have many empty cells. To maintain the relevance as much as I could, I check the probability of fail in missing values: it's merely the same as for the filled cells. So I imputed the values from the crossing of target0 curve and target1 curve.\par
The table contains many variables about client dwelling, I suppose aggregates from another table, as they have marks AVG, MODE and MEDI, and plenty of empty values. For every variable the kde curves are almost identical between var_AVG, var_MODE and var_MEDI. So I selected only those who had different curves for target0 and target1 (with AVG aggregation) and dropped the rest 40 variables.\par
\b\fs28 The dependent tables: bureau balance and bureau\par
\b0\fs22 Table bureau balance doesn't have missing values. I aggregated the balance by count (how many months the client holds an account), STATUS by get dummies (this variable is important because it shows if the client had DPD(days past due) in his credit history. When I join the bureau_balance to bureau on the given key, bureau table has some missing values. I filled the dummies with X (which is unknown status) and the balance with 0.\par
Bureau table has missing values in eight columns. To find out the best way to impute, I checked for each variable, how look the lined where this variables has a missing value. I didn't find that the proportion of the bad debt statues is higher. I also looked at the lined having CREDIT_ACTIVE variable bad_debt value (there are only 21). Half of them has the debt sum 0 or overdue days 0. So these variables aren't a good detector. I filled them with 0, because I thought to aggregate these columns by max or by sum. If I aggregate anything by mean, I would replace the missing values my mean value, in order not to shift the mean. When I joined the bureau to application_train on the given key, quite many values are missing. I filled the numeric columns with the median (because I'm not going to aggregate any more, and want to have a "neutral" value that doesn't shift the decision). Categoricals, I filled STATUS_X (unknown) and "unknown type of loan" with 1, the rest with 0.\par
Finally I selected features in some iterations: I compared the models on full set of variables (that took a long time, overnight), I saw which variables have high shap values and kept them (except if they are correlated). I tried to drop the variables that were not in the list and check whether it reduces performance of the model. It turns out, many features can be dropped without harm (like CNT_FAM_MEMBERS). Some are essential (like EXT_SOURCE), and some are middle_important (reduce the performance about 0.5%: it's a small difference but it's still there).\par
\b\fs32 Train test split, model comparison, hyperparameters selection\par
\b0\fs22 The class distribution is highly imbalanced: 8% of positive target. That's why I used stratified split by target, so that both train and test set have 8% of positive target. I kept 10% for test.\par
I compared 3 models: LogisticRegression, PassiveAggressiveClassifier and RandomForestClassifier. Logistic regression is a classic, PassiveAggressiveClassifier is known to work as good as LogisticRegression but is known to be better in eliminating redundant features (from where the name "aggressive"), and RandomForest is known to be very efficient, though tends to overfit. I wanted to try also the LightGBM algorithm, which is offerd in the Kernel (see "limits and improvements" section).\par
I selected hyperparameters with exhaustive GridSearch. As grid-search is time-consuming, I took only 1 or 2 key parameters, which are usually the most important, have the highest impact on model performance(see "limits and improvements"  for other grid methods for hyperparameter choice and for stratified KFold for sampling during cross_validation).\par
One issue to overcome during model testing is class imbalance. You can resolve this issue by setting the class_weight parameter "balanced", which just assigns the higher weights to the minor class samples, and which reveals to work the best. But you can use oversampling technique, abbreviated as SMOTE, which creates virtual samples of minor class, with the values of these new samples in the vectors connecting any chosen sample with random samples of the same class. SMOTE is said to work better in combination with undersampling techniques, such as Tomek Links or ENN (see "limits and improvements").\par
\par
\par
\par
\par
}
 